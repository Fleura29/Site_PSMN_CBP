.. _listex2go:

Plus de 50 machines pour se connecter à x2go ou SSH !
=====================================================

.. container:: text-center 

    .. container:: bg-warning-subtle pt-2 pb-1 mb-2 rounded fs-14
        
        L'analyse les logs de connexion aux stations de travail a montré que beaucoup de personnes 
        se connectaient sur les mêmes stations. Voici la liste détaillant toutes les machines 
        exploitables à distance

Ces machines sont accessibles par SSH et :ref:`x2go <x2go>` : '<nom>.cbp.ens-lyon.fr'

28 stations de travail en salle de formation
--------------------------------------------

Stations avec 1 processeur 6 coeurs à 2GHz, 32Go de RAM
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* **gtx690** : Nvidia GTX 690 avec 2x2Go
* **gtx680** : Nvidia GTX 680 avec 2Go, Nvidia GT1030 avec 2Go
* **gtx980** : Nvidia GTX 980 avec 4Go, Nvidia GT1030 avec 2Go
* **gtxtitan** : Nvidia GTX Titan avec 6Go, Nvidia K420 avec 2Go
* **gtx780ti** : Nvidia GTX 780Ti avec 3Go, Nvidia K2000 avec 2Go
* **q4000alpha** : Nvidia K4000 avec 3Go et capacité 3D
* **gtx960** : Nvidia GTX 960 avec 2Go, Nvidia GT710 avec 2Go
* **gtx970** : Nvidia GTX 970 avec 2Go, Nvidia GT1030 avec 2Go
* **gtx980tialpha** : Nvidia GTX 980Ti avec 6Go, Nvidia GTX 750Ti avec 2Go
* **gtx980tibeta** : Nvidia GTX 980Ti avec 6Go, Nvidia P600 avec 2Go
* **gtx980tidelta** : Nvidia GTX 980Ti avec 6Go, Nvidia K420 avec 1Go
* **gt730** : Nvidia GT 730 avec 2Go, Nvidia RTX 2080 avec 8Go

Stations avec 2 processeurs à 4 coeurs à 2.5GHz, 64Go de RAM
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* **gtx980beta** : Nvidia GTX 980 avec 4Go, Nvidia GT 1030 avec 2Go
* **gtx980tigamma** : Nvidia GTX980Ti avec 6Go,  Nvidia GTX 750 avec 2Go
* **k40** : Nvidia K2000 avec 2Go, Nvidia Tesla K40 avec 12Go
* **gt640** : Nvidia GT 640 avec 2Go, Nvidia RTX 2070 avec 8Go

Stations avec 2 processeurs à 8 coeurs à 2.4GHz, 32Go de RAM
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* **gtx780beta** : Nvidia GTX 780 avec 3Go, Nvidia K420 avec 2Go
* **gtx780gamma** : Nvidia GTX 780 avec 3Go, Nvidia K420 avec 2Go
* **gtx780delta** : Nvidia GTX 780 avec 3Go, Nvidia GT1030 avec 2Go
* **gtx1050ti** : Nvidia GTX 1050Ti avec 4Go, Nvidia P600 avec 2Go

Stations avec 2 processeurs à 10 coeurs à 2.2GHz, 64Go de RAM
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* **p600alpha** : Nvidia P600 avec 2Go, Nvidia GTX 1080 avec 8Go
* **p600beta** : Nvidia P600 avec 2Go, Nvidia GTX 780 avec 8Go

Stations avec 2 processeurs à 2 coeurs à 3.3GHz, 16Go de RAM
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* **o790alpha** : Nvidia NVS 315 avec 1Go
* **o790beta** : Nvidia GT620 avec 1Go

Stations avec 2 processeurs de 2 à 4 coeurs, 8Go de RAM
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* **o760alpha** : Nvidia GT 620 avec 1Go, Intel Q6600
* **o760beta** : Nvidia GT 620 avec 1Go, Intel E7500
* **o760gamma** : Nvidia GT 430 avec 1Go, Intel E5200
* **o760delta** : Nvidia GT 620 avec 1Go, Intel E5400

3 Machines ouvertes
-------------------

Stations avec 1 processeur à 8 coeurs à 4GHz, 64Go de RAM
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* **i9900ks** : Intel i9900KS, Nvidia RTX 2080Ti avec 12Go
* **ryzen3800** : AMD Ryzen 3900X, Nvidia RTX 2080Ti avec 12Go

Station avec 1 processeur à 64 coeurs à 1.3GHz, 96Go de RAM
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* **phi7210** : Intel Xeon Phi 7210 à 64 coeurs

13 Machines virtuelles à accélérateur
-------------------------------------

* **k40m** : Nvidia Tesla K40m avec 12GB

* **k80alpha** : 1/2 de Tesla K80 - 1GPU avec 12GB
* **k80beta** : 1/2 de Tesla K80 - 1GPU avec 12GB
* **k80gamma** : Tesla K80 - 2GPU avec 12GB

* **gtx1080alpha** : Nvidia GTX 1080 avec 8GB
* **gtx1080beta** : Nvidia GTX 1080 avec 8GB
* **gtx1080gamma** : Nvidia GTX 1080 avec 8GB
* **gtx1080delta** : Nvidia GTX 1080 avec 8GB

* **p100alpha** : Nvidia Tesla P100 avec 16GB
* **p100beta** : Nvidia Tesla P100 avec 16GB
* **p100gamma** : 2x Nvidia Tesla P100 avec 16GB

* **v100alpha** : Nvidia Tesla V100 avec 16GB
* **v100beta** : Nvidia Tesla V100 avec 16GB

6 Stations Deep Learning à accélérateur
---------------------------------------

Stations avec 1 processeur à 4 coeurs à 3.3GHz, 16Go de RAM
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* **openstation5** : 2 Nvidia GTX 780 avec 3GB, espace de stockage de 4TB en SSHD
* **openstation4** : 1 Nvidia GTX 780 avec 3GB, espace de stockage de 4TB

Stations avec 1 processeur à 4 coeurs à 3.5GHz, 32GB de RAM
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* **openstation6** : 1 RTX 2080 Super avec 8GB, 1 GTX 1080 avec 8GB, espace de stockage de 4TB en SSHD
* **openstation7** : 1 RTX 2080 Super avec 8GB, 1 GTX 1080 avec 8GB, espace de stockage de 4TB en SSHD
* **openstation8** : 1 RTX 2080 Super avec 8GB, 1 GTX 1080 avec 8GB, espace de stockage de 4TB en SSHD
* **openstation9** : 1 RTX 2080 Super avec 8GB, 1 GTX 1080 avec 8GB, espace de stockage de 4TB en SSHD

3 Stations rackables à accélérateur
-----------------------------------

Stations avec 1 processeur à 4 coeurs à 2.66GHz, 32GB de RAM
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* **r5400alpha** : Nvidia GTX 980 avec 6GB, espace de stockage de 1TB en Raid1
* **r5400beta**  : Nvidia GTX 780 avec 3GB, espace de stockage de 1TB en Raid1

Stations avec processeurs AMD Epyc Rome
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* **rome4gpu**   : 1x Epyc 7302 à 16 coeurs, 128 GB de RAM

    * 4x Nvidia RTX 2080 Super avec 8GB
    *  espace de stockage de 3TB
* **epyc1** : 2x Epyc 7502 à 32 coeurs, 256 GB de RAM

    * 1x Nvidia RTX 2080 Super avec 8GB 
    * espace de stockage de 10TB
* **epyc2** : 2x Epyc 7702 à 64 coeurs, 256 GB de RAM

    * 1x Nvidia RTX 2080 Super avec 8GB
    * espace de stockage de 10TB

6 noeuds de cluster rackables récents
-------------------------------------

Noeuds avec 4 processeurs de 8 à 12 coeurs à 2.3 GHz
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* **r815cores32** : 32 coeurs, 128 GB de RAM, 2.2 TB d'espace disque
* **r815cores32** : 48 coeurs, 192 GB de RAM, 4.4 TB d'espace disque

Noeuds avec 2 processeurs à 16 coeurs à 2.3GHz, de 192 GB à 2 TB de RAM
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* **apollo192g1** : 192 GB de RAM, 2 TB d'espace disque
* **apollo192g2** : 192 GB de RAM, 2 TB d'espace disque
* **apollo1024g** : 1024 GB de DCPMM, 192 GB de RAM cache, 2 TB d'espace disque
* **apollo2048g** : 2048 GB de DCPMM, 192 GB de RAM cache, 2 TB d'espace disque

Modalités d'accès
-----------------

Ces machines sont accessibles par SSH et :ref:`x2go <x2go>`: '<nom>.cbp.ens-lyon.fr'

Une vision d'ensemble de l'`état des stations de travail <http://styx.cbp.ens-lyon.fr/ganglia/?r=hour&c=Workstations>`_ sous :ref:`SIDUS <sidusdoc>` est accessible seulement de l'intérieur de l'ENS-Lyon.

Espaces de stockage
-------------------

L'utilisateur dispose sur ces machines de 5 espaces de stockage :

* son compte utilisateur "$HOME" : monté avec le protocole SMB, il ne doit pas être trop sollicité pour des calculs nécessitant de gros transferts. Un archivage (permettant de revenir sur l'état du volume dans le passé) est réalisé chaque nuit.
* l'espace temporaire "/tmp" : ce dossier est en mémoire vive. Rapide, il est raisonnable de ne pas trop le solliciter pour les gros volumes. Une fois la mémoire vive remplie, des dysfonctionnements peuvent apparaître.
* l'espace local "/local" : ce dossier correspond à un disque dur interne. La vitesse d'accès est d'une centaine de MB/s. Il n'est ni partagé, ni sauvegardé. Pour l'exploiter, créer un dossier correspondant à son identifiant : "mkdir /local/$USER"
* l'espace vrac "/scratch": ce dossier, partagé mais non sauvegardé entre toutes les machines, dispose d'un accès rapide sur le réseau de la salle et très rapide sur le réseau du cluster. Pour l'exploiter, créer un dossier correspondant à son identifiant : "mkdir /scratch/$USER"
* l'espace projets "/projects": ce dossier, archivé mais non sauvegardé entre toutes les machines, dispose d'un accès rapide sur le réseau de la salle et sur le réseau du cluster. Pour l'exploiter, créer un dossier correspondant à son identifiant : "mkdir /projects/users/$USER"
